2024-11-21 02:15:21,218 - INFO - Checkpoint directory: ./results/unbiased/online/16
2024-11-21 02:15:21,218 - INFO - Configuration:
2024-11-21 02:15:21,219 - INFO - sim_coeff: 25.0
2024-11-21 02:15:21,219 - INFO - std_coeff: 25
2024-11-21 02:15:21,219 - INFO - cov_coeff: 1
2024-11-21 02:15:21,219 - INFO - batch_sizes: [256, 128, 64, 32, 16, 8]
2024-11-21 02:15:21,219 - INFO - num_epochs: 50
2024-11-21 02:15:21,219 - INFO - max_lr_vicreg: 0.09375000000000001
2024-11-21 02:15:21,219 - INFO - momentum: 0.9
2024-11-21 02:15:21,219 - INFO - weight_decay: 0.0001
2024-11-21 02:15:21,219 - INFO - final_lr_schedule_value: 1.25e-05
2024-11-21 02:15:21,219 - INFO - warmup_epochs: 5
2024-11-21 02:15:21,219 - INFO - batch_size_evaluate: 16
2024-11-21 02:15:21,220 - INFO - num_eval_epochs: 50
2024-11-21 02:15:21,220 - INFO - max_lr_linear: 0.0048828125
2024-11-21 02:15:21,220 - INFO - linear_momentum: 0.9
2024-11-21 02:15:21,220 - INFO - linear_weight_decay: 0.0
2024-11-21 02:15:21,220 - INFO - backbone: resnet18
2024-11-21 02:15:21,220 - INFO - augs_train_type: lightly
2024-11-21 02:15:21,220 - INFO - augs_eval_enable: False
2024-11-21 02:15:21,220 - INFO - num_layers: 3
2024-11-21 02:15:21,220 - INFO - projection_head_dims: [512, 2048]
2024-11-21 02:15:21,220 - INFO - probe: online
2024-11-21 02:15:21,220 - INFO - loss: unbiased
2024-11-21 02:15:21,220 - INFO - batch_size_sharing: True
2024-11-21 02:15:21,220 - INFO - scale_lr_batched: True
2024-11-21 02:15:21,220 - INFO - batch_size: 16
2024-11-21 02:15:21,220 - INFO - checkpoint_dir: ./results/unbiased/online/16
2024-11-21 02:15:21,220 - INFO - Running with batch_size=16
2024-11-21 02:15:21,220 - INFO - Using device: cuda
2024-11-21 02:15:21,222 - INFO - Setting up experiment...
2024-11-21 02:15:21,222 - INFO - Using ResNet18 backbone
2024-11-21 02:15:21,497 - INFO - Using unbiased VICReg loss
2024-11-21 02:15:21,497 - INFO - Setting up datasets and dataloaders
2024-11-21 02:15:22,966 - INFO - Created dataloaders with batch size 16 and evaluate 16
2024-11-21 02:15:22,967 - INFO - Created optimizers with learning rates: vicreg=0.09375000000000001, linear=0.0048828125
2024-11-21 02:15:22,967 - INFO - Loaded checkpoints: vicreg_epoch=0, linear_epoch=0
2024-11-21 02:15:22,968 - INFO - Starting from epoch vicreg_start:0
2024-11-21 02:15:22,968 - INFO - Writing visualization data to TensorBoard
2024-11-21 02:15:24,265 - INFO - Successfully wrote visualization data to TensorBoard
2024-11-21 02:15:24,283 - INFO - Beginning train + evaluate for online probing
2024-11-21 02:15:24,284 - INFO - Continuing training from epoch 0 to 50
2024-11-21 02:18:21,930 - INFO - Epoch: 00, unbiasedVICReg loss: 143.66153, Train Loss: 2.24418, Train Acc: 17.46%
2024-11-21 02:18:21,931 - INFO - Epoch: 00, Invariance loss: 0.02095
2024-11-21 02:18:21,931 - INFO - Epoch: 00, Variance loss: 0.92460
2024-11-21 02:18:21,931 - INFO - Epoch: 00, Covariance loss: 0.30250
2024-11-21 02:18:21,931 - INFO - Epoch: 00, Compare losses: 23.94139 == 143.66153
2024-11-21 02:18:22,121 - INFO - Epoch: 00, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:18:26,636 - INFO - Epoch: 00, Test Loss: 2.27008, Test Acc: 15.75%
2024-11-21 02:21:19,049 - INFO - Epoch: 01, unbiasedVICReg loss: 45.16410, Train Loss: 2.07867, Train Acc: 27.82%
2024-11-21 02:21:19,049 - INFO - Epoch: 01, Invariance loss: 0.00205
2024-11-21 02:21:19,049 - INFO - Epoch: 01, Variance loss: 0.95222
2024-11-21 02:21:19,049 - INFO - Epoch: 01, Covariance loss: 0.00175
2024-11-21 02:21:19,049 - INFO - Epoch: 01, Compare losses: 23.85835 == 45.16410
2024-11-21 02:21:19,892 - INFO - Epoch: 01, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 02:21:24,002 - INFO - Epoch: 01, Test Loss: 2.16500, Test Acc: 19.65%
2024-11-21 02:24:18,959 - INFO - Epoch: 02, unbiasedVICReg loss: 45.15062, Train Loss: 1.97415, Train Acc: 32.40%
2024-11-21 02:24:18,960 - INFO - Epoch: 02, Invariance loss: 0.00183
2024-11-21 02:24:18,960 - INFO - Epoch: 02, Variance loss: 0.95008
2024-11-21 02:24:18,960 - INFO - Epoch: 02, Covariance loss: 0.00199
2024-11-21 02:24:18,960 - INFO - Epoch: 02, Compare losses: 23.79979 == 45.15062
2024-11-21 02:24:19,829 - INFO - Epoch: 02, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:24:24,010 - INFO - Epoch: 02, Test Loss: 2.13617, Test Acc: 19.40%
2024-11-21 02:27:16,761 - INFO - Epoch: 03, unbiasedVICReg loss: 45.14212, Train Loss: 1.91443, Train Acc: 34.77%
2024-11-21 02:27:16,762 - INFO - Epoch: 03, Invariance loss: 0.00166
2024-11-21 02:27:16,762 - INFO - Epoch: 03, Variance loss: 0.94896
2024-11-21 02:27:16,762 - INFO - Epoch: 03, Covariance loss: 0.00212
2024-11-21 02:27:16,762 - INFO - Epoch: 03, Compare losses: 23.76764 == 45.14212
2024-11-21 02:27:17,601 - INFO - Epoch: 03, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 02:27:21,811 - INFO - Epoch: 03, Test Loss: 2.12656, Test Acc: 18.24%
2024-11-21 02:30:18,874 - INFO - Epoch: 04, unbiasedVICReg loss: 45.13654, Train Loss: 1.86858, Train Acc: 36.13%
2024-11-21 02:30:18,875 - INFO - Epoch: 04, Invariance loss: 0.00154
2024-11-21 02:30:18,875 - INFO - Epoch: 04, Variance loss: 0.94827
2024-11-21 02:30:18,875 - INFO - Epoch: 04, Covariance loss: 0.00220
2024-11-21 02:30:18,875 - INFO - Epoch: 04, Compare losses: 23.74739 == 45.13654
2024-11-21 02:30:19,711 - INFO - Epoch: 04, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:30:24,028 - INFO - Epoch: 04, Test Loss: 2.19872, Test Acc: 22.40%
2024-11-21 02:33:16,283 - INFO - Epoch: 05, unbiasedVICReg loss: 45.13080, Train Loss: 1.84300, Train Acc: 36.75%
2024-11-21 02:33:16,284 - INFO - Epoch: 05, Invariance loss: 0.00139
2024-11-21 02:33:16,284 - INFO - Epoch: 05, Variance loss: 0.94755
2024-11-21 02:33:16,284 - INFO - Epoch: 05, Covariance loss: 0.00230
2024-11-21 02:33:16,284 - INFO - Epoch: 05, Compare losses: 23.72593 == 45.13080
2024-11-21 02:33:17,116 - INFO - Epoch: 05, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 02:33:21,221 - INFO - Epoch: 05, Test Loss: 1.92590, Test Acc: 31.86%
2024-11-21 02:36:17,182 - INFO - Epoch: 06, unbiasedVICReg loss: 45.12662, Train Loss: 1.81963, Train Acc: 37.08%
2024-11-21 02:36:17,182 - INFO - Epoch: 06, Invariance loss: 0.00129
2024-11-21 02:36:17,182 - INFO - Epoch: 06, Variance loss: 0.94709
2024-11-21 02:36:17,182 - INFO - Epoch: 06, Covariance loss: 0.00236
2024-11-21 02:36:17,182 - INFO - Epoch: 06, Compare losses: 23.71177 == 45.12662
2024-11-21 02:36:18,067 - INFO - Epoch: 06, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:36:23,104 - INFO - Epoch: 06, Test Loss: 1.95633, Test Acc: 28.97%
2024-11-21 02:39:16,517 - INFO - Epoch: 07, unbiasedVICReg loss: 45.12336, Train Loss: 1.79851, Train Acc: 37.78%
2024-11-21 02:39:16,517 - INFO - Epoch: 07, Invariance loss: 0.00121
2024-11-21 02:39:16,517 - INFO - Epoch: 07, Variance loss: 0.94672
2024-11-21 02:39:16,517 - INFO - Epoch: 07, Covariance loss: 0.00242
2024-11-21 02:39:16,517 - INFO - Epoch: 07, Compare losses: 23.70054 == 45.12336
2024-11-21 02:39:17,354 - INFO - Epoch: 07, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 02:39:21,676 - INFO - Epoch: 07, Test Loss: 1.97872, Test Acc: 26.31%
2024-11-21 02:42:20,451 - INFO - Epoch: 08, unbiasedVICReg loss: 45.12080, Train Loss: 1.77838, Train Acc: 38.57%
2024-11-21 02:42:20,451 - INFO - Epoch: 08, Invariance loss: 0.00115
2024-11-21 02:42:20,451 - INFO - Epoch: 08, Variance loss: 0.94641
2024-11-21 02:42:20,451 - INFO - Epoch: 08, Covariance loss: 0.00246
2024-11-21 02:42:20,451 - INFO - Epoch: 08, Compare losses: 23.69140 == 45.12080
2024-11-21 02:42:21,284 - INFO - Epoch: 08, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:42:25,623 - INFO - Epoch: 08, Test Loss: 1.83233, Test Acc: 35.49%
2024-11-21 02:45:16,230 - INFO - Epoch: 09, unbiasedVICReg loss: 45.11874, Train Loss: 1.76566, Train Acc: 39.07%
2024-11-21 02:45:16,230 - INFO - Epoch: 09, Invariance loss: 0.00110
2024-11-21 02:45:16,230 - INFO - Epoch: 09, Variance loss: 0.94616
2024-11-21 02:45:16,230 - INFO - Epoch: 09, Covariance loss: 0.00249
2024-11-21 02:45:16,230 - INFO - Epoch: 09, Compare losses: 23.68403 == 45.11874
2024-11-21 02:45:17,112 - INFO - Epoch: 09, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 02:45:21,219 - INFO - Epoch: 09, Test Loss: 1.91960, Test Acc: 30.41%
2024-11-21 02:48:17,108 - INFO - Epoch: 10, unbiasedVICReg loss: 45.11707, Train Loss: 1.74721, Train Acc: 39.55%
2024-11-21 02:48:17,108 - INFO - Epoch: 10, Invariance loss: 0.00106
2024-11-21 02:48:17,108 - INFO - Epoch: 10, Variance loss: 0.94597
2024-11-21 02:48:17,108 - INFO - Epoch: 10, Covariance loss: 0.00252
2024-11-21 02:48:17,108 - INFO - Epoch: 10, Compare losses: 23.67831 == 45.11707
2024-11-21 02:48:17,921 - INFO - Epoch: 10, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:48:22,084 - INFO - Epoch: 10, Test Loss: 1.82343, Test Acc: 35.59%
2024-11-21 02:51:15,725 - INFO - Epoch: 11, unbiasedVICReg loss: 45.11563, Train Loss: 1.73656, Train Acc: 40.27%
2024-11-21 02:51:15,726 - INFO - Epoch: 11, Invariance loss: 0.00102
2024-11-21 02:51:15,726 - INFO - Epoch: 11, Variance loss: 0.94579
2024-11-21 02:51:15,726 - INFO - Epoch: 11, Covariance loss: 0.00254
2024-11-21 02:51:15,726 - INFO - Epoch: 11, Compare losses: 23.67279 == 45.11563
2024-11-21 02:51:16,581 - INFO - Epoch: 11, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 02:51:20,847 - INFO - Epoch: 11, Test Loss: 1.82710, Test Acc: 34.74%
2024-11-21 02:54:16,670 - INFO - Epoch: 12, unbiasedVICReg loss: 45.11441, Train Loss: 1.71916, Train Acc: 40.90%
2024-11-21 02:54:16,670 - INFO - Epoch: 12, Invariance loss: 0.00099
2024-11-21 02:54:16,670 - INFO - Epoch: 12, Variance loss: 0.94564
2024-11-21 02:54:16,670 - INFO - Epoch: 12, Covariance loss: 0.00256
2024-11-21 02:54:16,670 - INFO - Epoch: 12, Compare losses: 23.66833 == 45.11441
2024-11-21 02:54:17,500 - INFO - Epoch: 12, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 02:54:21,856 - INFO - Epoch: 12, Test Loss: 1.78013, Test Acc: 37.62%
2024-11-21 02:57:13,989 - INFO - Epoch: 13, unbiasedVICReg loss: 45.11323, Train Loss: 1.71282, Train Acc: 40.99%
2024-11-21 02:57:13,989 - INFO - Epoch: 13, Invariance loss: 0.00097
2024-11-21 02:57:13,990 - INFO - Epoch: 13, Variance loss: 0.94551
2024-11-21 02:57:13,990 - INFO - Epoch: 13, Covariance loss: 0.00258
2024-11-21 02:57:13,990 - INFO - Epoch: 13, Compare losses: 23.66439 == 45.11323
2024-11-21 02:57:14,853 - INFO - Epoch: 13, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 02:57:19,153 - INFO - Epoch: 13, Test Loss: 1.69096, Test Acc: 41.57%
2024-11-21 03:00:15,897 - INFO - Epoch: 14, unbiasedVICReg loss: 45.11200, Train Loss: 1.69559, Train Acc: 41.78%
2024-11-21 03:00:15,897 - INFO - Epoch: 14, Invariance loss: 0.00094
2024-11-21 03:00:15,897 - INFO - Epoch: 14, Variance loss: 0.94537
2024-11-21 03:00:15,897 - INFO - Epoch: 14, Covariance loss: 0.00260
2024-11-21 03:00:15,897 - INFO - Epoch: 14, Compare losses: 23.66033 == 45.11200
2024-11-21 03:00:16,749 - INFO - Epoch: 14, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:00:21,139 - INFO - Epoch: 14, Test Loss: 1.72218, Test Acc: 39.77%
2024-11-21 03:03:13,898 - INFO - Epoch: 15, unbiasedVICReg loss: 45.11108, Train Loss: 1.68398, Train Acc: 42.36%
2024-11-21 03:03:13,899 - INFO - Epoch: 15, Invariance loss: 0.00091
2024-11-21 03:03:13,899 - INFO - Epoch: 15, Variance loss: 0.94529
2024-11-21 03:03:13,899 - INFO - Epoch: 15, Covariance loss: 0.00261
2024-11-21 03:03:13,899 - INFO - Epoch: 15, Compare losses: 23.65755 == 45.11108
2024-11-21 03:03:14,744 - INFO - Epoch: 15, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 03:03:19,618 - INFO - Epoch: 15, Test Loss: 1.76684, Test Acc: 37.99%
2024-11-21 03:06:18,219 - INFO - Epoch: 16, unbiasedVICReg loss: 45.11009, Train Loss: 1.67298, Train Acc: 42.86%
2024-11-21 03:06:18,220 - INFO - Epoch: 16, Invariance loss: 0.00089
2024-11-21 03:06:18,220 - INFO - Epoch: 16, Variance loss: 0.94518
2024-11-21 03:06:18,220 - INFO - Epoch: 16, Covariance loss: 0.00263
2024-11-21 03:06:18,220 - INFO - Epoch: 16, Compare losses: 23.65427 == 45.11009
2024-11-21 03:06:19,066 - INFO - Epoch: 16, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:06:23,792 - INFO - Epoch: 16, Test Loss: 1.74640, Test Acc: 37.85%
2024-11-21 03:09:18,101 - INFO - Epoch: 17, unbiasedVICReg loss: 45.10957, Train Loss: 1.66181, Train Acc: 43.16%
2024-11-21 03:09:18,101 - INFO - Epoch: 17, Invariance loss: 0.00088
2024-11-21 03:09:18,101 - INFO - Epoch: 17, Variance loss: 0.94510
2024-11-21 03:09:18,101 - INFO - Epoch: 17, Covariance loss: 0.00264
2024-11-21 03:09:18,102 - INFO - Epoch: 17, Compare losses: 23.65193 == 45.10957
2024-11-21 03:09:18,940 - INFO - Epoch: 17, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 03:09:23,412 - INFO - Epoch: 17, Test Loss: 1.67797, Test Acc: 41.58%
2024-11-21 03:12:19,296 - INFO - Epoch: 18, unbiasedVICReg loss: 45.10876, Train Loss: 1.65523, Train Acc: 43.26%
2024-11-21 03:12:19,297 - INFO - Epoch: 18, Invariance loss: 0.00085
2024-11-21 03:12:19,297 - INFO - Epoch: 18, Variance loss: 0.94501
2024-11-21 03:12:19,297 - INFO - Epoch: 18, Covariance loss: 0.00265
2024-11-21 03:12:19,297 - INFO - Epoch: 18, Compare losses: 23.64930 == 45.10876
2024-11-21 03:12:20,147 - INFO - Epoch: 18, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:12:24,682 - INFO - Epoch: 18, Test Loss: 1.69310, Test Acc: 41.38%
2024-11-21 03:15:19,289 - INFO - Epoch: 19, unbiasedVICReg loss: 45.10830, Train Loss: 1.64243, Train Acc: 44.10%
2024-11-21 03:15:19,289 - INFO - Epoch: 19, Invariance loss: 0.00085
2024-11-21 03:15:19,289 - INFO - Epoch: 19, Variance loss: 0.94495
2024-11-21 03:15:19,289 - INFO - Epoch: 19, Covariance loss: 0.00266
2024-11-21 03:15:19,289 - INFO - Epoch: 19, Compare losses: 23.64747 == 45.10830
2024-11-21 03:15:20,105 - INFO - Epoch: 19, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 03:15:24,474 - INFO - Epoch: 19, Test Loss: 1.76427, Test Acc: 35.76%
2024-11-21 03:18:20,340 - INFO - Epoch: 20, unbiasedVICReg loss: 45.10758, Train Loss: 1.62794, Train Acc: 44.48%
2024-11-21 03:18:20,340 - INFO - Epoch: 20, Invariance loss: 0.00083
2024-11-21 03:18:20,340 - INFO - Epoch: 20, Variance loss: 0.94487
2024-11-21 03:18:20,340 - INFO - Epoch: 20, Covariance loss: 0.00267
2024-11-21 03:18:20,340 - INFO - Epoch: 20, Compare losses: 23.64507 == 45.10758
2024-11-21 03:18:21,214 - INFO - Epoch: 20, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:18:25,624 - INFO - Epoch: 20, Test Loss: 1.68460, Test Acc: 41.34%
2024-11-21 03:21:20,198 - INFO - Epoch: 21, unbiasedVICReg loss: 45.10687, Train Loss: 1.62307, Train Acc: 44.63%
2024-11-21 03:21:20,199 - INFO - Epoch: 21, Invariance loss: 0.00081
2024-11-21 03:21:20,199 - INFO - Epoch: 21, Variance loss: 0.94479
2024-11-21 03:21:20,199 - INFO - Epoch: 21, Covariance loss: 0.00268
2024-11-21 03:21:20,199 - INFO - Epoch: 21, Compare losses: 23.64265 == 45.10687
2024-11-21 03:21:21,038 - INFO - Epoch: 21, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 03:21:25,193 - INFO - Epoch: 21, Test Loss: 1.65898, Test Acc: 43.40%
2024-11-21 03:24:21,275 - INFO - Epoch: 22, unbiasedVICReg loss: 45.10641, Train Loss: 1.61508, Train Acc: 44.90%
2024-11-21 03:24:21,275 - INFO - Epoch: 22, Invariance loss: 0.00080
2024-11-21 03:24:21,275 - INFO - Epoch: 22, Variance loss: 0.94475
2024-11-21 03:24:21,275 - INFO - Epoch: 22, Covariance loss: 0.00268
2024-11-21 03:24:21,275 - INFO - Epoch: 22, Compare losses: 23.64137 == 45.10641
2024-11-21 03:24:22,125 - INFO - Epoch: 22, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:24:26,619 - INFO - Epoch: 22, Test Loss: 1.70317, Test Acc: 40.39%
2024-11-21 03:27:18,012 - INFO - Epoch: 23, unbiasedVICReg loss: 45.10574, Train Loss: 1.60952, Train Acc: 45.60%
2024-11-21 03:27:18,012 - INFO - Epoch: 23, Invariance loss: 0.00078
2024-11-21 03:27:18,012 - INFO - Epoch: 23, Variance loss: 0.94466
2024-11-21 03:27:18,012 - INFO - Epoch: 23, Covariance loss: 0.00270
2024-11-21 03:27:18,012 - INFO - Epoch: 23, Compare losses: 23.63869 == 45.10574
2024-11-21 03:27:18,864 - INFO - Epoch: 23, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 03:27:23,190 - INFO - Epoch: 23, Test Loss: 1.64259, Test Acc: 42.58%
2024-11-21 03:30:19,114 - INFO - Epoch: 24, unbiasedVICReg loss: 45.10516, Train Loss: 1.59804, Train Acc: 46.24%
2024-11-21 03:30:19,115 - INFO - Epoch: 24, Invariance loss: 0.00077
2024-11-21 03:30:19,115 - INFO - Epoch: 24, Variance loss: 0.94461
2024-11-21 03:30:19,115 - INFO - Epoch: 24, Covariance loss: 0.00270
2024-11-21 03:30:19,115 - INFO - Epoch: 24, Compare losses: 23.63716 == 45.10516
2024-11-21 03:30:19,975 - INFO - Epoch: 24, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:30:24,321 - INFO - Epoch: 24, Test Loss: 1.65485, Test Acc: 41.84%
2024-11-21 03:33:15,844 - INFO - Epoch: 25, unbiasedVICReg loss: 45.10513, Train Loss: 1.59155, Train Acc: 46.18%
2024-11-21 03:33:15,844 - INFO - Epoch: 25, Invariance loss: 0.00077
2024-11-21 03:33:15,844 - INFO - Epoch: 25, Variance loss: 0.94460
2024-11-21 03:33:15,844 - INFO - Epoch: 25, Covariance loss: 0.00270
2024-11-21 03:33:15,845 - INFO - Epoch: 25, Compare losses: 23.63686 == 45.10513
2024-11-21 03:33:16,701 - INFO - Epoch: 25, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 03:33:20,976 - INFO - Epoch: 25, Test Loss: 1.61466, Test Acc: 43.77%
2024-11-21 03:36:16,181 - INFO - Epoch: 26, unbiasedVICReg loss: 45.10463, Train Loss: 1.58538, Train Acc: 46.44%
2024-11-21 03:36:16,181 - INFO - Epoch: 26, Invariance loss: 0.00076
2024-11-21 03:36:16,181 - INFO - Epoch: 26, Variance loss: 0.94453
2024-11-21 03:36:16,181 - INFO - Epoch: 26, Covariance loss: 0.00271
2024-11-21 03:36:16,181 - INFO - Epoch: 26, Compare losses: 23.63488 == 45.10463
2024-11-21 03:36:17,050 - INFO - Epoch: 26, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:36:21,229 - INFO - Epoch: 26, Test Loss: 1.61453, Test Acc: 45.17%
2024-11-21 03:39:13,076 - INFO - Epoch: 27, unbiasedVICReg loss: 45.10421, Train Loss: 1.58033, Train Acc: 46.82%
2024-11-21 03:39:13,077 - INFO - Epoch: 27, Invariance loss: 0.00075
2024-11-21 03:39:13,077 - INFO - Epoch: 27, Variance loss: 0.94451
2024-11-21 03:39:13,077 - INFO - Epoch: 27, Covariance loss: 0.00271
2024-11-21 03:39:13,077 - INFO - Epoch: 27, Compare losses: 23.63402 == 45.10421
2024-11-21 03:39:13,948 - INFO - Epoch: 27, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 03:39:18,630 - INFO - Epoch: 27, Test Loss: 1.59577, Test Acc: 46.23%
2024-11-21 03:42:15,007 - INFO - Epoch: 28, unbiasedVICReg loss: 45.10364, Train Loss: 1.57607, Train Acc: 46.68%
2024-11-21 03:42:15,007 - INFO - Epoch: 28, Invariance loss: 0.00073
2024-11-21 03:42:15,007 - INFO - Epoch: 28, Variance loss: 0.94443
2024-11-21 03:42:15,007 - INFO - Epoch: 28, Covariance loss: 0.00273
2024-11-21 03:42:15,007 - INFO - Epoch: 28, Compare losses: 23.63179 == 45.10364
2024-11-21 03:42:15,882 - INFO - Epoch: 28, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:42:20,188 - INFO - Epoch: 28, Test Loss: 1.55890, Test Acc: 47.13%
2024-11-21 03:45:13,399 - INFO - Epoch: 29, unbiasedVICReg loss: 45.10338, Train Loss: 1.56752, Train Acc: 47.54%
2024-11-21 03:45:13,400 - INFO - Epoch: 29, Invariance loss: 0.00072
2024-11-21 03:45:13,400 - INFO - Epoch: 29, Variance loss: 0.94440
2024-11-21 03:45:13,400 - INFO - Epoch: 29, Covariance loss: 0.00273
2024-11-21 03:45:13,400 - INFO - Epoch: 29, Compare losses: 23.63087 == 45.10338
2024-11-21 03:45:14,255 - INFO - Epoch: 29, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 03:45:18,381 - INFO - Epoch: 29, Test Loss: 1.54705, Test Acc: 47.88%
2024-11-21 03:48:14,908 - INFO - Epoch: 30, unbiasedVICReg loss: 45.10300, Train Loss: 1.55663, Train Acc: 47.59%
2024-11-21 03:48:14,908 - INFO - Epoch: 30, Invariance loss: 0.00072
2024-11-21 03:48:14,908 - INFO - Epoch: 30, Variance loss: 0.94435
2024-11-21 03:48:14,908 - INFO - Epoch: 30, Covariance loss: 0.00274
2024-11-21 03:48:14,908 - INFO - Epoch: 30, Compare losses: 23.62947 == 45.10300
2024-11-21 03:48:15,750 - INFO - Epoch: 30, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:48:20,055 - INFO - Epoch: 30, Test Loss: 1.53204, Test Acc: 48.03%
2024-11-21 03:51:11,590 - INFO - Epoch: 31, unbiasedVICReg loss: 45.10257, Train Loss: 1.54283, Train Acc: 48.40%
2024-11-21 03:51:11,590 - INFO - Epoch: 31, Invariance loss: 0.00070
2024-11-21 03:51:11,590 - INFO - Epoch: 31, Variance loss: 0.94432
2024-11-21 03:51:11,590 - INFO - Epoch: 31, Covariance loss: 0.00274
2024-11-21 03:51:11,590 - INFO - Epoch: 31, Compare losses: 23.62836 == 45.10257
2024-11-21 03:51:12,427 - INFO - Epoch: 31, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 03:51:16,814 - INFO - Epoch: 31, Test Loss: 1.55236, Test Acc: 47.13%
2024-11-21 03:54:12,914 - INFO - Epoch: 32, unbiasedVICReg loss: 45.10237, Train Loss: 1.54084, Train Acc: 48.24%
2024-11-21 03:54:12,914 - INFO - Epoch: 32, Invariance loss: 0.00070
2024-11-21 03:54:12,914 - INFO - Epoch: 32, Variance loss: 0.94428
2024-11-21 03:54:12,914 - INFO - Epoch: 32, Covariance loss: 0.00274
2024-11-21 03:54:12,914 - INFO - Epoch: 32, Compare losses: 23.62734 == 45.10237
2024-11-21 03:54:13,783 - INFO - Epoch: 32, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 03:54:17,962 - INFO - Epoch: 32, Test Loss: 1.56431, Test Acc: 46.89%
2024-11-21 03:57:10,698 - INFO - Epoch: 33, unbiasedVICReg loss: 45.10215, Train Loss: 1.53504, Train Acc: 48.52%
2024-11-21 03:57:10,699 - INFO - Epoch: 33, Invariance loss: 0.00069
2024-11-21 03:57:10,699 - INFO - Epoch: 33, Variance loss: 0.94427
2024-11-21 03:57:10,699 - INFO - Epoch: 33, Covariance loss: 0.00275
2024-11-21 03:57:10,699 - INFO - Epoch: 33, Compare losses: 23.62676 == 45.10215
2024-11-21 03:57:11,538 - INFO - Epoch: 33, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 03:57:15,728 - INFO - Epoch: 33, Test Loss: 1.56703, Test Acc: 46.37%
2024-11-21 04:00:11,660 - INFO - Epoch: 34, unbiasedVICReg loss: 45.10194, Train Loss: 1.52541, Train Acc: 49.00%
2024-11-21 04:00:11,660 - INFO - Epoch: 34, Invariance loss: 0.00069
2024-11-21 04:00:11,661 - INFO - Epoch: 34, Variance loss: 0.94424
2024-11-21 04:00:11,661 - INFO - Epoch: 34, Covariance loss: 0.00275
2024-11-21 04:00:11,661 - INFO - Epoch: 34, Compare losses: 23.62600 == 45.10194
2024-11-21 04:00:12,527 - INFO - Epoch: 34, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:00:16,780 - INFO - Epoch: 34, Test Loss: 1.52616, Test Acc: 48.04%
2024-11-21 04:03:08,991 - INFO - Epoch: 35, unbiasedVICReg loss: 45.10124, Train Loss: 1.51402, Train Acc: 49.83%
2024-11-21 04:03:08,991 - INFO - Epoch: 35, Invariance loss: 0.00067
2024-11-21 04:03:08,992 - INFO - Epoch: 35, Variance loss: 0.94417
2024-11-21 04:03:08,992 - INFO - Epoch: 35, Covariance loss: 0.00276
2024-11-21 04:03:08,992 - INFO - Epoch: 35, Compare losses: 23.62382 == 45.10124
2024-11-21 04:03:09,953 - INFO - Epoch: 35, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 04:03:14,321 - INFO - Epoch: 35, Test Loss: 1.53977, Test Acc: 48.09%
2024-11-21 04:06:10,702 - INFO - Epoch: 36, unbiasedVICReg loss: 45.10114, Train Loss: 1.51071, Train Acc: 49.86%
2024-11-21 04:06:10,702 - INFO - Epoch: 36, Invariance loss: 0.00067
2024-11-21 04:06:10,702 - INFO - Epoch: 36, Variance loss: 0.94414
2024-11-21 04:06:10,703 - INFO - Epoch: 36, Covariance loss: 0.00277
2024-11-21 04:06:10,703 - INFO - Epoch: 36, Compare losses: 23.62300 == 45.10114
2024-11-21 04:06:11,537 - INFO - Epoch: 36, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:06:15,823 - INFO - Epoch: 36, Test Loss: 1.47654, Test Acc: 51.23%
2024-11-21 04:09:11,012 - INFO - Epoch: 37, unbiasedVICReg loss: 45.10078, Train Loss: 1.50455, Train Acc: 49.92%
2024-11-21 04:09:11,012 - INFO - Epoch: 37, Invariance loss: 0.00066
2024-11-21 04:09:11,012 - INFO - Epoch: 37, Variance loss: 0.94412
2024-11-21 04:09:11,012 - INFO - Epoch: 37, Covariance loss: 0.00277
2024-11-21 04:09:11,013 - INFO - Epoch: 37, Compare losses: 23.62233 == 45.10078
2024-11-21 04:09:11,917 - INFO - Epoch: 37, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 04:09:16,133 - INFO - Epoch: 37, Test Loss: 1.53488, Test Acc: 47.82%
2024-11-21 04:12:11,869 - INFO - Epoch: 38, unbiasedVICReg loss: 45.10057, Train Loss: 1.49839, Train Acc: 50.17%
2024-11-21 04:12:11,869 - INFO - Epoch: 38, Invariance loss: 0.00066
2024-11-21 04:12:11,869 - INFO - Epoch: 38, Variance loss: 0.94410
2024-11-21 04:12:11,869 - INFO - Epoch: 38, Covariance loss: 0.00277
2024-11-21 04:12:11,869 - INFO - Epoch: 38, Compare losses: 23.62159 == 45.10057
2024-11-21 04:12:12,728 - INFO - Epoch: 38, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:12:17,110 - INFO - Epoch: 38, Test Loss: 1.48887, Test Acc: 50.83%
2024-11-21 04:15:12,974 - INFO - Epoch: 39, unbiasedVICReg loss: 45.10045, Train Loss: 1.48949, Train Acc: 50.45%
2024-11-21 04:15:12,974 - INFO - Epoch: 39, Invariance loss: 0.00065
2024-11-21 04:15:12,975 - INFO - Epoch: 39, Variance loss: 0.94408
2024-11-21 04:15:12,975 - INFO - Epoch: 39, Covariance loss: 0.00277
2024-11-21 04:15:12,975 - INFO - Epoch: 39, Compare losses: 23.62112 == 45.10045
2024-11-21 04:15:13,820 - INFO - Epoch: 39, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 04:15:18,137 - INFO - Epoch: 39, Test Loss: 1.44020, Test Acc: 52.13%
2024-11-21 04:18:12,454 - INFO - Epoch: 40, unbiasedVICReg loss: 45.10018, Train Loss: 1.48743, Train Acc: 50.73%
2024-11-21 04:18:12,455 - INFO - Epoch: 40, Invariance loss: 0.00065
2024-11-21 04:18:12,455 - INFO - Epoch: 40, Variance loss: 0.94404
2024-11-21 04:18:12,455 - INFO - Epoch: 40, Covariance loss: 0.00278
2024-11-21 04:18:12,455 - INFO - Epoch: 40, Compare losses: 23.62002 == 45.10018
2024-11-21 04:18:13,311 - INFO - Epoch: 40, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:18:17,923 - INFO - Epoch: 40, Test Loss: 1.46462, Test Acc: 50.93%
2024-11-21 04:21:14,362 - INFO - Epoch: 41, unbiasedVICReg loss: 45.10004, Train Loss: 1.47885, Train Acc: 51.11%
2024-11-21 04:21:14,362 - INFO - Epoch: 41, Invariance loss: 0.00064
2024-11-21 04:21:14,362 - INFO - Epoch: 41, Variance loss: 0.94403
2024-11-21 04:21:14,362 - INFO - Epoch: 41, Covariance loss: 0.00278
2024-11-21 04:21:14,362 - INFO - Epoch: 41, Compare losses: 23.61948 == 45.10004
2024-11-21 04:21:15,232 - INFO - Epoch: 41, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 04:21:19,825 - INFO - Epoch: 41, Test Loss: 1.49613, Test Acc: 49.92%
2024-11-21 04:24:16,039 - INFO - Epoch: 42, unbiasedVICReg loss: 45.09965, Train Loss: 1.47574, Train Acc: 51.13%
2024-11-21 04:24:16,039 - INFO - Epoch: 42, Invariance loss: 0.00063
2024-11-21 04:24:16,039 - INFO - Epoch: 42, Variance loss: 0.94398
2024-11-21 04:24:16,039 - INFO - Epoch: 42, Covariance loss: 0.00279
2024-11-21 04:24:16,039 - INFO - Epoch: 42, Compare losses: 23.61816 == 45.09965
2024-11-21 04:24:16,898 - INFO - Epoch: 42, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:24:21,351 - INFO - Epoch: 42, Test Loss: 1.44283, Test Acc: 52.12%
2024-11-21 04:27:18,414 - INFO - Epoch: 43, unbiasedVICReg loss: 45.09968, Train Loss: 1.46950, Train Acc: 51.55%
2024-11-21 04:27:18,414 - INFO - Epoch: 43, Invariance loss: 0.00063
2024-11-21 04:27:18,414 - INFO - Epoch: 43, Variance loss: 0.94398
2024-11-21 04:27:18,414 - INFO - Epoch: 43, Covariance loss: 0.00279
2024-11-21 04:27:18,414 - INFO - Epoch: 43, Compare losses: 23.61803 == 45.09968
2024-11-21 04:27:19,261 - INFO - Epoch: 43, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 04:27:23,641 - INFO - Epoch: 43, Test Loss: 1.50465, Test Acc: 48.16%
2024-11-21 04:30:15,105 - INFO - Epoch: 44, unbiasedVICReg loss: 45.09923, Train Loss: 1.46201, Train Acc: 51.56%
2024-11-21 04:30:15,106 - INFO - Epoch: 44, Invariance loss: 0.00062
2024-11-21 04:30:15,106 - INFO - Epoch: 44, Variance loss: 0.94394
2024-11-21 04:30:15,106 - INFO - Epoch: 44, Covariance loss: 0.00279
2024-11-21 04:30:15,106 - INFO - Epoch: 44, Compare losses: 23.61678 == 45.09923
2024-11-21 04:30:15,934 - INFO - Epoch: 44, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:30:20,490 - INFO - Epoch: 44, Test Loss: 1.43818, Test Acc: 51.80%
2024-11-21 04:33:17,144 - INFO - Epoch: 45, unbiasedVICReg loss: 45.09912, Train Loss: 1.46175, Train Acc: 51.50%
2024-11-21 04:33:17,144 - INFO - Epoch: 45, Invariance loss: 0.00062
2024-11-21 04:33:17,144 - INFO - Epoch: 45, Variance loss: 0.94393
2024-11-21 04:33:17,144 - INFO - Epoch: 45, Covariance loss: 0.00279
2024-11-21 04:33:17,145 - INFO - Epoch: 45, Compare losses: 23.61644 == 45.09912
2024-11-21 04:33:18,002 - INFO - Epoch: 45, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 04:33:22,260 - INFO - Epoch: 45, Test Loss: 1.39886, Test Acc: 54.32%
2024-11-21 04:36:15,983 - INFO - Epoch: 46, unbiasedVICReg loss: 45.09900, Train Loss: 1.45371, Train Acc: 51.90%
2024-11-21 04:36:15,983 - INFO - Epoch: 46, Invariance loss: 0.00061
2024-11-21 04:36:15,983 - INFO - Epoch: 46, Variance loss: 0.94390
2024-11-21 04:36:15,983 - INFO - Epoch: 46, Covariance loss: 0.00280
2024-11-21 04:36:15,983 - INFO - Epoch: 46, Compare losses: 23.61573 == 45.09900
2024-11-21 04:36:16,835 - INFO - Epoch: 46, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:36:20,949 - INFO - Epoch: 46, Test Loss: 1.43966, Test Acc: 52.73%
2024-11-21 04:39:21,972 - INFO - Epoch: 47, unbiasedVICReg loss: 45.09881, Train Loss: 1.44911, Train Acc: 52.15%
2024-11-21 04:39:21,973 - INFO - Epoch: 47, Invariance loss: 0.00061
2024-11-21 04:39:21,973 - INFO - Epoch: 47, Variance loss: 0.94391
2024-11-21 04:39:21,973 - INFO - Epoch: 47, Covariance loss: 0.00280
2024-11-21 04:39:21,973 - INFO - Epoch: 47, Compare losses: 23.61572 == 45.09881
2024-11-21 04:39:22,797 - INFO - Epoch: 47, Optimizer LR: 0.09375000, Linear Optimizer LR: 0.00004883
2024-11-21 04:39:27,367 - INFO - Epoch: 47, Test Loss: 1.39736, Test Acc: 53.29%
2024-11-21 04:42:21,576 - INFO - Epoch: 48, unbiasedVICReg loss: 45.09870, Train Loss: 1.43732, Train Acc: 52.47%
2024-11-21 04:42:21,576 - INFO - Epoch: 48, Invariance loss: 0.00061
2024-11-21 04:42:21,576 - INFO - Epoch: 48, Variance loss: 0.94388
2024-11-21 04:42:21,576 - INFO - Epoch: 48, Covariance loss: 0.00280
2024-11-21 04:42:21,576 - INFO - Epoch: 48, Compare losses: 23.61503 == 45.09870
2024-11-21 04:42:22,432 - INFO - Epoch: 48, Optimizer LR: 0.04688125, Linear Optimizer LR: 0.00004883
2024-11-21 04:42:26,602 - INFO - Epoch: 48, Test Loss: 1.39738, Test Acc: 54.27%
2024-11-21 04:45:23,338 - INFO - Epoch: 49, unbiasedVICReg loss: 45.09858, Train Loss: 1.43025, Train Acc: 52.87%
2024-11-21 04:45:23,339 - INFO - Epoch: 49, Invariance loss: 0.00061
2024-11-21 04:45:23,339 - INFO - Epoch: 49, Variance loss: 0.94386
2024-11-21 04:45:23,339 - INFO - Epoch: 49, Covariance loss: 0.00280
2024-11-21 04:45:23,339 - INFO - Epoch: 49, Compare losses: 23.61449 == 45.09858
2024-11-21 04:45:24,191 - INFO - Epoch: 49, Optimizer LR: 0.00001250, Linear Optimizer LR: 0.00004883
2024-11-21 04:45:28,353 - INFO - Epoch: 49, Test Loss: 1.39986, Test Acc: 52.75%
